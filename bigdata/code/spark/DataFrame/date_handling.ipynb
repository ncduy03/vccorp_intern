{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-28T04:14:02.704344Z",
     "start_time": "2025-03-28T04:14:02.688268Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "spark = (SparkSession.builder.appName(\"date_handling\")\n",
    "         .master(\"local[*]\")\n",
    "         .config(\"spark.executor.memory\", \"1g\")\n",
    "         .getOrCreate()\n",
    "         )"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T04:14:02.720012Z",
     "start_time": "2025-03-28T04:14:02.713959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a CSV file\n",
    "csv_data = \"\"\"id,date_iso,date_dmy,date_mdy,timestamp\n",
    "1,2023-01-15,15/01/2023,01/15/2023,2023-01-15 10:30:00\n",
    "2,2023-05-20,20/05/2023,05/20/2023,2023-05-20 15:45:00\n",
    "3,InvalidDate,31/02/2023,02/31/2023,InvalidTimestamp\n",
    "4,,,\n",
    "\"\"\"\n",
    "\n",
    "# Save the CSV file\n",
    "with open(\"dates_data.csv\", \"w\") as f:\n",
    "    f.write(csv_data)"
   ],
   "id": "be815720810d9f74",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T04:14:10.054661Z",
     "start_time": "2025-03-28T04:14:02.736143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sample data with multiple date formats\n",
    "data = [\n",
    "    (1, \"2023-01-15\", \"15/01/2023\", \"01/15/2023\", \"2023-01-15 10:30:00\"),\n",
    "    (2, \"2023-05-20\", \"20/05/2023\", \"05/20/2023\", \"2023-05-20 15:45:00\"),\n",
    "    (3, \"InvalidDate\", \"31/02/2023\", \"02/31/2023\", \"InvalidTimestamp\"),  # Invalid dates\n",
    "    (4, None, None, None, None)  # Null values\n",
    "]\n",
    "\n",
    "# Define column names\n",
    "columns = [\"id\", \"date_iso\", \"date_dmy\", \"date_mdy\", \"timestamp\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)"
   ],
   "id": "353121207f51fe49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------+----------+-------------------+\n",
      "|id |date_iso   |date_dmy  |date_mdy  |timestamp          |\n",
      "+---+-----------+----------+----------+-------------------+\n",
      "|1  |2023-01-15 |15/01/2023|01/15/2023|2023-01-15 10:30:00|\n",
      "|2  |2023-05-20 |20/05/2023|05/20/2023|2023-05-20 15:45:00|\n",
      "|3  |InvalidDate|31/02/2023|02/31/2023|InvalidTimestamp   |\n",
      "|4  |NULL       |NULL      |NULL      |NULL               |\n",
      "+---+-----------+----------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T04:14:10.104026Z",
     "start_time": "2025-03-28T04:14:10.087979Z"
    }
   },
   "cell_type": "code",
   "source": "df.printSchema()",
   "id": "af45c4a72ebc398c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- date_iso: string (nullable = true)\n",
      " |-- date_dmy: string (nullable = true)\n",
      " |-- date_mdy: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T04:14:10.213809Z",
     "start_time": "2025-03-28T04:14:10.198178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from pyspark.sql.types import *\n",
    "# # StructType for the schema\n",
    "# struct_schema = StructType([\n",
    "#     StructField(\"id\", IntegerType(), nullable=True),\n",
    "#     StructField(\"date_iso\", StringType(), nullable=True),\n",
    "#     StructField(\"date_dmy\", StringType(), nullable=True),\n",
    "#     StructField(\"date_mdy\", StringType(), nullable=True),\n",
    "#     StructField(\"timestamp\", StringType(), nullable=True)\n",
    "# ])"
   ],
   "id": "6708d5476933393f",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T04:14:10.391745Z",
     "start_time": "2025-03-28T04:14:10.262244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DDL String for the schema\n",
    "ddl_schema = \"\"\"\n",
    "    id INT,\n",
    "    date_iso DATE,\n",
    "    date_dmy DATE,\n",
    "    date_mdy DATE,\n",
    "    timestamp TIMESTAMP\n",
    "\"\"\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df_file = spark.read.option(\"header\", True).schema(ddl_schema).csv(\"dates_data.csv\")\n",
    "\n",
    "# Show the DataFrame\n",
    "df_file.show(truncate=False)"
   ],
   "id": "7c1a3ddbd1120733",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+--------+-------------------+\n",
      "|id |date_iso  |date_dmy|date_mdy|timestamp          |\n",
      "+---+----------+--------+--------+-------------------+\n",
      "|1  |2023-01-15|NULL    |NULL    |2023-01-15 10:30:00|\n",
      "|2  |2023-05-20|NULL    |NULL    |2023-05-20 15:45:00|\n",
      "|3  |NULL      |NULL    |NULL    |NULL               |\n",
      "|4  |NULL      |NULL    |NULL    |NULL               |\n",
      "+---+----------+--------+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T04:14:10.441396Z",
     "start_time": "2025-03-28T04:14:10.422270Z"
    }
   },
   "cell_type": "code",
   "source": "df_file.printSchema()",
   "id": "84d53ab26de7a46f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date_iso: date (nullable = true)\n",
      " |-- date_dmy: date (nullable = true)\n",
      " |-- date_mdy: date (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T04:14:10.587944Z",
     "start_time": "2025-03-28T04:14:10.529227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "df = (df\n",
    "      .withColumn('parsed_date_iso', to_date('date_iso', 'yyyy-MM-dd'))\n",
    "      .withColumn('parsed_date_dmy', to_date('date_dmy', 'dd/MM/yyyy'))\n",
    "      .withColumn('parsed_date_mdy', to_date('date_mdy', 'MM/dd/yyyy')))"
   ],
   "id": "698ab361d62d8a72",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T04:14:18.558123Z",
     "start_time": "2025-03-28T04:14:10.670254Z"
    }
   },
   "cell_type": "code",
   "source": "df.show(truncate=False)",
   "id": "c9f382313a264471",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------+----------+-------------------+---------------+---------------+---------------+\n",
      "|id |date_iso   |date_dmy  |date_mdy  |timestamp          |parsed_date_iso|parsed_date_dmy|parsed_date_mdy|\n",
      "+---+-----------+----------+----------+-------------------+---------------+---------------+---------------+\n",
      "|1  |2023-01-15 |15/01/2023|01/15/2023|2023-01-15 10:30:00|2023-01-15     |2023-01-15     |2023-01-15     |\n",
      "|2  |2023-05-20 |20/05/2023|05/20/2023|2023-05-20 15:45:00|2023-05-20     |2023-05-20     |2023-05-20     |\n",
      "|3  |InvalidDate|31/02/2023|02/31/2023|InvalidTimestamp   |NULL           |NULL           |NULL           |\n",
      "|4  |NULL       |NULL      |NULL      |NULL               |NULL           |NULL           |NULL           |\n",
      "+---+-----------+----------+----------+-------------------+---------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Timestamp",
   "id": "5aeae27be53f8875"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T04:14:38.062766Z",
     "start_time": "2025-03-28T04:14:38.049189Z"
    }
   },
   "cell_type": "code",
   "source": "from pyspark.sql.functions import *",
   "id": "f243459dee35fd24",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T04:16:49.651761Z",
     "start_time": "2025-03-28T04:16:41.550097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = df.withColumn('parsed_timestamp', to_timestamp(df.timestamp, 'yyyy-MM-dd HH:mm:ss'))\n",
    "df.show()\n",
    "df.printSchema()"
   ],
   "id": "2f6e4fac91f82f53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------+----------+-------------------+---------------+---------------+---------------+-------------------+\n",
      "| id|   date_iso|  date_dmy|  date_mdy|          timestamp|parsed_date_iso|parsed_date_dmy|parsed_date_mdy|   parsed_timestamp|\n",
      "+---+-----------+----------+----------+-------------------+---------------+---------------+---------------+-------------------+\n",
      "|  1| 2023-01-15|15/01/2023|01/15/2023|2023-01-15 10:30:00|     2023-01-15|     2023-01-15|     2023-01-15|2023-01-15 10:30:00|\n",
      "|  2| 2023-05-20|20/05/2023|05/20/2023|2023-05-20 15:45:00|     2023-05-20|     2023-05-20|     2023-05-20|2023-05-20 15:45:00|\n",
      "|  3|InvalidDate|31/02/2023|02/31/2023|   InvalidTimestamp|           NULL|           NULL|           NULL|               NULL|\n",
      "|  4|       NULL|      NULL|      NULL|               NULL|           NULL|           NULL|           NULL|               NULL|\n",
      "+---+-----------+----------+----------+-------------------+---------------+---------------+---------------+-------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- date_iso: string (nullable = true)\n",
      " |-- date_dmy: string (nullable = true)\n",
      " |-- date_mdy: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- parsed_date_iso: date (nullable = true)\n",
      " |-- parsed_date_dmy: date (nullable = true)\n",
      " |-- parsed_date_mdy: date (nullable = true)\n",
      " |-- parsed_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T04:18:50.525618Z",
     "start_time": "2025-03-28T04:18:42.311356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = (df\n",
    "      .withColumn('year', year('parsed_timestamp'))\n",
    "      .withColumn('month', month('parsed_timestamp'))\n",
    "      .withColumn('day', day('parsed_timestamp'))\n",
    "      .withColumn('hour', hour('parsed_timestamp'))\n",
    "      .withColumn('minute', minute('parsed_timestamp'))\n",
    "      )\n",
    "df.show()"
   ],
   "id": "6837b99d0c0df21f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------+----------+-------------------+---------------+---------------+---------------+-------------------+----+-----+----+----+------+\n",
      "| id|   date_iso|  date_dmy|  date_mdy|          timestamp|parsed_date_iso|parsed_date_dmy|parsed_date_mdy|   parsed_timestamp|year|month| day|hour|minute|\n",
      "+---+-----------+----------+----------+-------------------+---------------+---------------+---------------+-------------------+----+-----+----+----+------+\n",
      "|  1| 2023-01-15|15/01/2023|01/15/2023|2023-01-15 10:30:00|     2023-01-15|     2023-01-15|     2023-01-15|2023-01-15 10:30:00|2023|    1|  15|  10|    30|\n",
      "|  2| 2023-05-20|20/05/2023|05/20/2023|2023-05-20 15:45:00|     2023-05-20|     2023-05-20|     2023-05-20|2023-05-20 15:45:00|2023|    5|  20|  15|    45|\n",
      "|  3|InvalidDate|31/02/2023|02/31/2023|   InvalidTimestamp|           NULL|           NULL|           NULL|               NULL|NULL| NULL|NULL|NULL|  NULL|\n",
      "|  4|       NULL|      NULL|      NULL|               NULL|           NULL|           NULL|           NULL|               NULL|NULL| NULL|NULL|NULL|  NULL|\n",
      "+---+-----------+----------+----------+-------------------+---------------+---------------+---------------+-------------------+----+-----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T04:21:23.637701Z",
     "start_time": "2025-03-28T04:21:15.420888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = df.withColumn('days_difference', datediff(current_date(), 'parsed_date_iso'))\n",
    "df.select('parsed_date_mdy', 'parsed_date_iso', 'days_difference').show()"
   ],
   "id": "349c57a8ea2d0d93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+---------------+\n",
      "|parsed_date_mdy|parsed_date_iso|days_difference|\n",
      "+---------------+---------------+---------------+\n",
      "|     2023-01-15|     2023-01-15|            803|\n",
      "|     2023-05-20|     2023-05-20|            678|\n",
      "|           NULL|           NULL|           NULL|\n",
      "|           NULL|           NULL|           NULL|\n",
      "+---------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7a7a120e81ba79a7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
